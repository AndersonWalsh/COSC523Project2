{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9910000,"sourceType":"datasetVersion","datasetId":6088972},{"sourceId":6058,"sourceType":"modelInstanceVersion","modelInstanceId":4679,"modelId":2819}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport keras_nlp\nfrom sklearn.model_selection import train_test_split\nfrom keras_tuner import Hyperband\n\ntrain_data = pd.read_csv(\"/kaggle/input/disaster-nlp/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disaster-nlp/test.csv\")\n\ntrain_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T23:44:44.891208Z","iopub.execute_input":"2024-11-14T23:44:44.891806Z","iopub.status.idle":"2024-11-14T23:44:44.951451Z","shell.execute_reply.started":"2024-11-14T23:44:44.891726Z","shell.execute_reply":"2024-11-14T23:44:44.950092Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import keras_nlp\n\n\ntokenizer = keras_nlp.tokenizers.BertTokenizer.from_preset(\"bert_base_en\")\n\n# Function to preprocess text data\ndef preprocess_text(text):\n    tokens = tokenizer.tokenize(text)\n    detokenized_text = tokenizer.detokenize(tokens)\n    return detokenized_text\n\ndf_train['text'] = df_train['text'].apply(preprocess_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T23:44:44.953304Z","iopub.execute_input":"2024-11-14T23:44:44.953730Z","iopub.status.idle":"2024-11-14T23:46:55.062651Z","shell.execute_reply.started":"2024-11-14T23:44:44.953684Z","shell.execute_reply":"2024-11-14T23:46:55.061210Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = keras_nlp.tokenizers.BertTokenizer.from_preset(\"bert_base_en\")\n\ndef preprocess_text(text):\n    token_ids = tokenizer(text).numpy().flatten()\n    padding_mask = [1] * len(token_ids)  \n    segment_ids = [0] * len(token_ids)\n    \n    return token_ids, padding_mask, segment_ids\n\ninput_ids_list = []\nattention_mask_list = []\ntoken_type_ids_list = []\n\nfor text in df_train[\"text\"]:\n    input_ids, attention_mask, token_type_ids = preprocess_text(text)\n    input_ids_list.append(input_ids)\n    attention_mask_list.append(attention_mask)\n    token_type_ids_list.append(token_type_ids)\n\ntoken_ids_list = []\npadding_mask_list = []\nsegment_ids_list = []\n\nfor text in df_train[\"text\"]:\n    token_ids, padding_mask, segment_ids = preprocess_text(text)\n    token_ids_list.append(token_ids)\n    padding_mask_list.append(padding_mask)\n    segment_ids_list.append(segment_ids)\n\ntoken_ids_padded = pad_sequences(token_ids_list, maxlen=max_len, padding='post', truncating='post')\npadding_mask_padded = pad_sequences(padding_mask_list, maxlen=max_len, padding='post', truncating='post')\nsegment_ids_padded = pad_sequences(segment_ids_list, maxlen=max_len, padding='post', truncating='post')\n\nX_train = {\n    \"token_ids\": np.array(token_ids_padded),\n    \"padding_mask\": np.array(padding_mask_padded),\n    \"segment_ids\": np.array(segment_ids_padded)\n}\ny_train = df_train[\"target\"].values\n\n#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T00:40:33.274685Z","iopub.execute_input":"2024-11-15T00:40:33.275266Z","iopub.status.idle":"2024-11-15T00:44:25.707262Z","shell.execute_reply.started":"2024-11-15T00:40:33.275215Z","shell.execute_reply":"2024-11-15T00:44:25.705687Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport keras_nlp\n\ndef build_model(hp):\n\n    bert_encoder = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en\")\n\n    token_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"token_ids\")\n    padding_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"padding_mask\")\n    segment_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    bert_output = bert_encoder({\n        \"token_ids\": token_ids, \n        \"padding_mask\": padding_mask, \n        \"segment_ids\": segment_ids\n    })\n\n    x = layers.GlobalAveragePooling1D()(bert_output[\"sequence_output\"])\n    x = layers.Dense(hp.Int(\"units\", min_value=64, max_value=256, step=64), activation=\"relu\")(x)\n    x = layers.Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1))(x)\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n\n    model = keras.Model(inputs=[token_ids, padding_mask, segment_ids], outputs=output)\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-5, 3e-5, 5e-5])),\n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T00:47:09.934161Z","iopub.execute_input":"2024-11-15T00:47:09.934634Z","iopub.status.idle":"2024-11-15T00:47:09.948304Z","shell.execute_reply.started":"2024-11-15T00:47:09.934592Z","shell.execute_reply":"2024-11-15T00:47:09.946609Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"token_ids_input = np.array(X_train[\"token_ids\"])\npadding_mask_input = np.array(X_train[\"padding_mask\"])\nsegment_ids_input = np.array(X_train[\"segment_ids\"])\n\ntuner = Hyperband(\n    build_model,\n    objective=\"val_accuracy\",\n    max_epochs=10,\n    factor=3,\n    directory=\"hyperband\",\n    project_name=\"disaster_tweets\"\n)\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\ntuner.search(\n    [token_ids_input, padding_mask_input, segment_ids_input],\n    y_train,\n    epochs=10,\n    validation_split=0.2,\n    callbacks=[stop_early]\n)\n\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T00:56:17.433528Z","iopub.execute_input":"2024-11-15T00:56:17.434910Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Trial 4 Complete [03h 32m 51s]\nval_accuracy: 0.829940915107727\n\nBest val_accuracy So Far: 0.829940915107727\nTotal elapsed time: 07h 47m 24s\n\nSearch: Running Trial #5\n\nValue             |Best Value So Far |Hyperparameter\n192               |192               |units\n0.3               |0.4               |dropout\n5e-05             |3e-05             |learning_rate\n2                 |2                 |tuner/epochs\n0                 |0                 |tuner/initial_epoch\n2                 |2                 |tuner/bracket\n0                 |0                 |tuner/round\n\nEpoch 1/2\n\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6420s\u001b[0m 33s/step - accuracy: 0.7327 - loss: 0.5351 - val_accuracy: 0.8194 - val_loss: 0.4103\nEpoch 2/2\n\u001b[1m 88/191\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m53:46\u001b[0m 31s/step - accuracy: 0.8856 - loss: 0.3101","output_type":"stream"}]},{"cell_type":"code","source":"history = best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n\nval_loss, val_accuracy = best_model.evaluate(X_val, y_val)\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T23:46:57.275294Z","iopub.status.idle":"2024-11-14T23:46:57.275894Z","shell.execute_reply.started":"2024-11-14T23:46:57.275574Z","shell.execute_reply":"2024-11-14T23:46:57.275604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T23:46:57.277533Z","iopub.status.idle":"2024-11-14T23:46:57.277991Z","shell.execute_reply.started":"2024-11-14T23:46:57.277738Z","shell.execute_reply":"2024-11-14T23:46:57.277758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[\"text\"] = test_data[\"text\"].apply(preprocess_text)\nX_test = np.array([tokenize_text(text)[\"input_ids\"].numpy().flatten() for text in test_data[\"text\"]])\n\npredictions = (best_model.predict(X_test) > 0.5).astype(\"int32\")\nsubmission = pd.DataFrame({\"id\": test_data[\"id\"], \"target\": predictions.flatten()})\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T23:46:57.279400Z","iopub.status.idle":"2024-11-14T23:46:57.279848Z","shell.execute_reply.started":"2024-11-14T23:46:57.279606Z","shell.execute_reply":"2024-11-14T23:46:57.279626Z"},"trusted":true},"execution_count":null,"outputs":[]}]}